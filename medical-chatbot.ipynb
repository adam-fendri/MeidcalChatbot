{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T14:51:03.360407Z","iopub.status.busy":"2024-08-17T14:51:03.360068Z","iopub.status.idle":"2024-08-17T14:55:39.312339Z","shell.execute_reply":"2024-08-17T14:55:39.310928Z","shell.execute_reply.started":"2024-08-17T14:51:03.360378Z"},"trusted":true},"outputs":[],"source":["%%capture\n","%pip uninstall pysqlite3 -y\n","%pip install pysqlite3-binary\n","\n","%pip install -U transformers bitsandbytes huggingface_hub langchain_community langchain_text_splitters chromadb\n","%pip install -U langchain langchain-chroma\n","%pip install faiss-cpu\n","%pip install sentence-transformers\n","%pip install -U langchain-huggingface\n","%pip install -U peft torch transformers huggingface_hub\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-17T15:03:02.773177Z","iopub.status.busy":"2024-08-17T15:03:02.772268Z","iopub.status.idle":"2024-08-17T15:04:50.570154Z","shell.execute_reply":"2024-08-17T15:04:50.569187Z","shell.execute_reply.started":"2024-08-17T15:03:02.773144Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"293c37b991614601ba7fe03d29ef521e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"name":"stdout","output_type":"stream","text":["Generated Prompt:\n","Human: \n","You are an AI Medical Assistant trained on a vast dataset of health information. Please be thorough and provide an informative answer. Answer the following question based on the provided context. If the context is irrelevant to the question, do not use it in your response. If you are unsure about a medical inquiry, advise seeking professional help.\n","\n","1\n","Cardiovascular Diseases\n","Aortic Dissection\n","IEssentials of Diagnosis\n","•Most patients between age 50 and age 70; risks include hyper-\n","tension, Marfan ’s syndrome, bicuspid aortic valve, coarctation of\n","the aorta, and pregnancy\n","•Type A involves the ascending aorta or arch; type B does not\n","\n","---\n","\n","Ann Thorac Surg 2000;69:1496. [PMID: 10881829]Chapter 1 Cardiovascular Diseases 5\n","1\n","\n","---\n","\n","Question: I weigh 70 kg, my height is 175 cm. I am 25 years old. Is it considered healthy for my age?\n","Answer:\n","\n","Question: I weigh 70 kg, my height is 175 cm. I am 25 years old. Is it considered healthy for my age?\n","Answer: Human: \n","You are an AI Medical Assistant trained on a vast dataset of health information. Please be thorough and provide an informative answer. Answer the following question based on the provided context. If the context is irrelevant to the question, do not use it in your response. If you are unsure about a medical inquiry, advise seeking professional help.\n","\n","1\n","Cardiovascular Diseases\n","Aortic Dissection\n","IEssentials of Diagnosis\n","•Most patients between age 50 and age 70; risks include hyper-\n","tension, Marfan ’s syndrome, bicuspid aortic valve, coarctation of\n","the aorta, and pregnancy\n","•Type A involves the ascending aorta or arch; type B does not\n","\n","---\n","\n","Ann Thorac Surg 2000;69:1496. [PMID: 10881829]Chapter 1 Cardiovascular Diseases 5\n","1\n","\n","---\n","\n","Question: I weigh 70 kg, my height is 175 cm. I am 25 years old. Is it considered healthy for my age?\n","Answer:\n","Hi,Thanks for writing in.You are a healthy young man. You have a normal weight and height. You are not overweight and your BMI is 22.5. This is a normal weight for your height. You are not obese and your weight is normal for your age. You are not over weight and your BMI is 22.5. This is a normal weight for your height. You are not overweight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight\n","NER Entities Detected: {\n","    \"age\": \".\",\n","    \"height\": \"height is\",\n","    \"weight\": \"70 kg\"\n","}\n","Chatbot Response: Human: \n","You are an AI Medical Assistant trained on a vast dataset of health information. Please be thorough and provide an informative answer. Answer the following question based on the provided context. If the context is irrelevant to the question, do not use it in your response. If you are unsure about a medical inquiry, advise seeking professional help.\n","\n","1\n","Cardiovascular Diseases\n","Aortic Dissection\n","IEssentials of Diagnosis\n","•Most patients between age 50 and age 70; risks include hyper-\n","tension, Marfan ’s syndrome, bicuspid aortic valve, coarctation of\n","the aorta, and pregnancy\n","•Type A involves the ascending aorta or arch; type B does not\n","\n","---\n","\n","Ann Thorac Surg 2000;69:1496. [PMID: 10881829]Chapter 1 Cardiovascular Diseases 5\n","1\n","\n","---\n","\n","Question: I weigh 70 kg, my height is 175 cm. I am 25 years old. Is it considered healthy for my age?\n","Answer:\n","Hi,Thanks for writing in.You are a healthy young man. You have a normal weight and height. You are not overweight and your BMI is 22.5. This is a normal weight for your height. You are not obese and your weight is normal for your age. You are not over weight and your BMI is 22.5. This is a normal weight for your height. You are not overweight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight and your weight is normal for your age. You are not over weight\n","Conversation Summary: I weigh 70 kg, my height is 175 cm. I am 25 years old. Is it considered healthy for my age? answer based on the provided context. if the context is irrelevant, do not use it . you are a healthy young man. you have a normal weight and height .\n"]}],"source":["import torch\n","from transformers import RobertaTokenizerFast, RobertaForTokenClassification, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, BitsAndBytesConfig, pipeline\n","import json\n","from huggingface_hub import login\n","import os\n","from langchain.document_loaders import PyPDFDirectoryLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.schema import Document\n","from langchain.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain.prompts import ChatPromptTemplate\n","from peft import PeftModel, PeftConfig\n","\n","# Define paths\n","DATA_PATH = \"/kaggle/input/10pages\"\n","FAISS_INDEX_PATH = \"/kaggle/working/faiss_index\"\n","\n","# Setup device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Login to Hugging Face using your API token\n","hf_token = \"\"\n","login(hf_token)\n","\n","# Load the RoBERTa NER model\n","ner_model = RobertaForTokenClassification.from_pretrained('adamfendri/robertaL_ner', use_auth_token=hf_token).to(device)\n","ner_tokenizer = RobertaTokenizerFast.from_pretrained('adamfendri/robertaL_ner', use_auth_token=hf_token)\n","\n","# 4-bit Quantization Configuration\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16,\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","# Load your fine-tuned Gemma-2 model with 4-bit quantization\n","config = PeftConfig.from_pretrained(\"adamfendri/Gemma-2-2b-it-medical\")\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    \"google/gemma-2-2b\",\n","    quantization_config=bnb_config,\n","    device_map={\"\": device},  # Ensure the model is loaded on the correct device\n",")\n","model = PeftModel.from_pretrained(base_model, \"adamfendri/Gemma-2-2b-it-medical\").to(device)\n","\n","# Load the tokenizer for Gemma-2\n","tokenizer = AutoTokenizer.from_pretrained(\"adamfendri/Gemma-2-2b-it-medical\", use_auth_token=hf_token)\n","\n","# Load the T5-Large model for summarization\n","summarizer_tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n","summarizer_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\").to(device)\n","\n","# Initialize the summarization pipeline\n","summarizer = pipeline(\"summarization\", model=summarizer_model, tokenizer=summarizer_tokenizer, device=device.index if device.type == 'cuda' else -1)\n","\n","# Define the prompt template\n","PROMPT_TEMPLATE = \"\"\"\n","You are an AI Medical Assistant trained on a vast dataset of health information. Please be thorough and provide an informative answer. Answer the following question based on the provided context. If the context is irrelevant to the question, do not use it in your response. If you are unsure about a medical inquiry, advise seeking professional help.\n","\n","{context}\n","\n","---\n","\n","Question: {question}\n","Answer:\n","\"\"\"\n","\n","# Function to get embedding function\n","def get_embedding_function():\n","    return HuggingFaceEmbeddings()\n","\n","# Function to load documents from PDF\n","def load_documents():\n","    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n","    return document_loader.load()\n","\n","# Function to split documents\n","def split_documents(documents):\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=350,\n","        chunk_overlap=50,\n","    )\n","    return text_splitter.split_documents(documents)\n","\n","# Function to add documents to FAISS index\n","def add_to_faiss(chunks):\n","    if os.path.exists(FAISS_INDEX_PATH):\n","        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings=get_embedding_function(), allow_dangerous_deserialization=True)\n","    else:\n","        db = FAISS.from_documents(chunks, embedding=get_embedding_function())\n","    \n","    db.save_local(FAISS_INDEX_PATH)\n","\n","# Function to process user input with NER model\n","def process_with_ner_model(user_input):\n","    inputs = ner_tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, is_split_into_words=False).to(device)\n","    with torch.no_grad():\n","        outputs = ner_model(**inputs)\n","    predictions = torch.argmax(outputs.logits, dim=-1)\n","\n","    # Define the label map\n","    id_to_label = {0: 'O', 1: 'B-WEIGHT', 2: 'I-WEIGHT', 3: 'B-HEIGHT', 4: 'I-HEIGHT', 5: 'B-AGE', 6: 'I-AGE'}\n","\n","    # Extract predicted labels and the corresponding tokens\n","    tokens = ner_tokenizer.tokenize(user_input)\n","    pred_labels = [id_to_label.get(pred.item(), 'O') for pred in predictions[0]]\n","\n","    # Improved Entity Extraction Logic\n","    def extract_entities(predicted_labels):\n","        entities = {\"age\": \"\", \"height\": \"\", \"weight\": \"\"}\n","        current_entity = None\n","        current_value = []\n","\n","        for token, label in zip(tokens, pred_labels):\n","            if token.startswith(\"Ġ\"):  # Handle space\n","                token = token[1:]  # Remove the special character for space\n","\n","            if label.startswith(\"B-\"):\n","                if current_entity:\n","                    # Join the tokens for the previous entity\n","                    entities[current_entity.lower()] = \" \".join(current_value).strip()\n","                current_entity = label[2:]  # Start new entity\n","                current_value = [token]\n","            elif label.startswith(\"I-\") and current_entity:\n","                current_value.append(token)  # Continue the entity\n","            else:\n","                if current_entity:\n","                    # Finalize the current entity\n","                    entities[current_entity.lower()] = \" \".join(current_value).strip()\n","                current_entity = None\n","                current_value = []\n","\n","        # Ensure the last entity is added\n","        if current_entity:\n","            entities[current_entity.lower()] = \" \".join(current_value).strip()\n","\n","        return entities\n","\n","    return extract_entities(pred_labels)\n","\n","# Function to query the RAG system and interact with the chatbot\n","def query_rag(query_text: str):\n","    # Prepare the FAISS index\n","    embedding_function = get_embedding_function()\n","    db = FAISS.load_local(FAISS_INDEX_PATH, embeddings=embedding_function, allow_dangerous_deserialization=True)\n","\n","    # Search the index\n","    results = db.similarity_search_with_score(query_text, k=2)\n","\n","    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n","    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n","    prompt = prompt_template.format(context=context_text, question=query_text)\n","    print(\"Generated Prompt:\")\n","    print(prompt)  # Check the generated prompt for correctness\n","\n","    # Use Gemma-2 model for RAG\n","    try:\n","        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n","        with torch.no_grad():\n","            response = model.generate(inputs.input_ids.to(device), max_length=512)\n","        response_text = tokenizer.decode(response[0], skip_special_tokens=True)\n","        sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n","        formatted_response = f\"Question: {query_text}\\nAnswer: {response_text}\"\n","        print(formatted_response)\n","        return response_text\n","\n","    except Exception as e:\n","        print(f\"Error invoking Gemma-2 model: {e}\")\n","        return None\n","\n","# Function to summarize the conversation\n","def summarize_conversation(question_text, response_text):\n","    # Only pass the user question and AI response text to the summarizer\n","    conversation_text = f\"Question: {question_text}\\nAnswer: {response_text}\"\n","    summary = summarizer(conversation_text, max_length=200, min_length=50, do_sample=False)\n","    return summary[0]['summary_text']\n","\n","# Function to populate the FAISS index\n","def populate_db():\n","    documents = load_documents()\n","    chunks = split_documents(documents)\n","    add_to_faiss(chunks)\n","\n","# Main function to run RAG and NER with summarization\n","def main(user_input):\n","    # Step 1: NER Extraction\n","    ner_entities = process_with_ner_model(user_input)\n","\n","    # Step 2: RAG Query with Gemma-2\n","    chatbot_response = query_rag(user_input)\n","\n","    # Step 3: Summarization\n","    summary = summarize_conversation(user_input, chatbot_response)\n","\n","    # Print Outputs\n","    print(\"NER Entities Detected:\", json.dumps(ner_entities, indent=4))\n","    print(\"Chatbot Response:\", chatbot_response)\n","    print(\"Conversation Summary:\", summary)\n","\n","# Example Interaction\n","if __name__ == \"__main__\":\n","    # Populate the database first\n","    populate_db()\n","\n","    # Example interaction\n","    user_input = \"I weigh 70 kg, my height is 175 cm. I am 25 years old. Is it considered healthy for my age?\"\n","    main(user_input)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5546907,"sourceId":9177898,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
