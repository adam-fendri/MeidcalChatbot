{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8e5ea3-0ab9-4c73-ae0f-5308ad53fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install necessary packages\n",
    "%pip install -U transformers \n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes \n",
    "%pip install -U wandb  # Install W&B for monitoring\n",
    "%pip install -U scikit-learn \n",
    "%pip install -U flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8a52fe-a6e9-481a-8b9f-c8ea9de312d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madamfendri\u001b[0m (\u001b[33madam-fendri\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240815_220649-obaaiqu1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adam-fendri/gemma2-finetuning/runs/obaaiqu1' target=\"_blank\">cool-wood-3</a></strong> to <a href='https://wandb.ai/adam-fendri/gemma2-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adam-fendri/gemma2-finetuning' target=\"_blank\">https://wandb.ai/adam-fendri/gemma2-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adam-fendri/gemma2-finetuning/runs/obaaiqu1' target=\"_blank\">https://wandb.ai/adam-fendri/gemma2-finetuning/runs/obaaiqu1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eaad64869444be087d93906f783b457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o_proj', 'down_proj', 'up_proj', 'k_proj', 'v_proj', 'gate_proj', 'q_proj']\n",
      "Trainable: 20766720 | total: 2635108608 | Percentage: 0.7881%\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import wandb  # Import W&B for monitoring\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, TrainerCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score  # Import scikit-learn metrics\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "huggingface_token = \"\"\n",
    "login(token=huggingface_token)\n",
    "\n",
    "# Initialize W&B and log in\n",
    "wandb.login(key=\"\")\n",
    "wandb.init(project=\"gemma2-finetuning\", entity=\"adam-fendri\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "base_model = \"google/gemma-2-2b\"\n",
    "new_model = \"Gemma-2-2b-it-medical\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'  # Set padding side to right\n",
    "\n",
    "# Determine CUDA device capabilities and use eager attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"eager\"  # Fallback to eager attention for stability\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "\n",
    "# QLoRA configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Identify target modules for LoRA\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if any(isinstance(module, cls) for cls in (torch.nn.Linear, torch.nn.Conv2d)):  # Check for linear layers\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "\n",
    "# Setup model for training\n",
    "model = get_peft_model(model, peft_config)\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f7bb77-faf8-4f9e-81c9-fc1f536f6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire dataset\n",
    "dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\", split=\"train\")\n",
    "\n",
    "# Shuffle and split dataset\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset = dataset.train_test_split(test_size=0.20, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "# Format the dataset with the chat template\n",
    "def format_chat_template(row):\n",
    "    input_text = f\"Context: {row['Patient']}\\nQuestion: {row['Description']}\\nAnswer: {row['Doctor']}\\n\"\n",
    "    row[\"text\"] = input_text\n",
    "    return row\n",
    "\n",
    "train_dataset = train_dataset.map(format_chat_template, num_proc=2)\n",
    "eval_dataset = eval_dataset.map(format_chat_template, num_proc=2)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=2)\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=True, num_proc=2)\n",
    "\n",
    "# Remove raw text columns\n",
    "train_dataset = train_dataset.remove_columns(['Description', 'Patient', 'Doctor', 'text'])\n",
    "eval_dataset = eval_dataset.remove_columns(['Description', 'Patient', 'Doctor', 'text'])\n",
    "\n",
    "# Custom callback to sample different evaluation subsets\n",
    "class RandomSubsetEvalCallback(TrainerCallback):\n",
    "    def __init__(self, eval_dataset, subset_size):\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % args.eval_steps == 0:\n",
    "            indices = random.sample(range(len(self.eval_dataset)), self.subset_size)\n",
    "            subset = self.eval_dataset.select(indices)\n",
    "            trainer.evaluate(eval_dataset=subset)\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9c9912-18f5-436e-8241-c911a5654df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='77073' max='77073' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [77073/77073 31:53:17, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.485000</td>\n",
       "      <td>2.294738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.303400</td>\n",
       "      <td>2.258224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.262700</td>\n",
       "      <td>2.226064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.240100</td>\n",
       "      <td>2.198877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.228300</td>\n",
       "      <td>2.270314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.196600</td>\n",
       "      <td>2.201096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.196000</td>\n",
       "      <td>2.249150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.180900</td>\n",
       "      <td>2.180279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.175700</td>\n",
       "      <td>2.135563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.177300</td>\n",
       "      <td>2.134840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.162700</td>\n",
       "      <td>2.143120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.154300</td>\n",
       "      <td>2.131050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.145900</td>\n",
       "      <td>2.096179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.145700</td>\n",
       "      <td>2.141682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.151500</td>\n",
       "      <td>2.190103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.142800</td>\n",
       "      <td>2.105605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.147500</td>\n",
       "      <td>2.121175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.135800</td>\n",
       "      <td>2.153626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.115200</td>\n",
       "      <td>2.106123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.134200</td>\n",
       "      <td>2.123229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.129300</td>\n",
       "      <td>2.146726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.116100</td>\n",
       "      <td>2.115178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.136700</td>\n",
       "      <td>2.156015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.112300</td>\n",
       "      <td>2.139005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.113000</td>\n",
       "      <td>2.060477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.122200</td>\n",
       "      <td>2.051965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.064100</td>\n",
       "      <td>2.144690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.038500</td>\n",
       "      <td>2.093979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.044700</td>\n",
       "      <td>2.119866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>2.035500</td>\n",
       "      <td>2.121021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>2.046400</td>\n",
       "      <td>2.070831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.025300</td>\n",
       "      <td>2.086630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>2.047500</td>\n",
       "      <td>2.162985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>2.033600</td>\n",
       "      <td>2.063871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>2.042600</td>\n",
       "      <td>2.101800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>2.032500</td>\n",
       "      <td>2.136831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>2.034800</td>\n",
       "      <td>2.072950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>2.042100</td>\n",
       "      <td>2.069191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>2.045400</td>\n",
       "      <td>2.075047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.025200</td>\n",
       "      <td>2.072948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>2.036400</td>\n",
       "      <td>2.126263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>2.041400</td>\n",
       "      <td>2.132830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>2.029900</td>\n",
       "      <td>2.043556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>2.045000</td>\n",
       "      <td>2.116628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>2.031800</td>\n",
       "      <td>2.127990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>2.040900</td>\n",
       "      <td>2.150616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>2.029600</td>\n",
       "      <td>2.074876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>2.032400</td>\n",
       "      <td>2.028020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>2.029400</td>\n",
       "      <td>2.109212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>2.040900</td>\n",
       "      <td>2.083709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>2.030600</td>\n",
       "      <td>2.098240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>2.022700</td>\n",
       "      <td>2.029235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>1.949900</td>\n",
       "      <td>2.073596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>1.950700</td>\n",
       "      <td>2.112723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>1.953900</td>\n",
       "      <td>2.125556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>1.951400</td>\n",
       "      <td>2.062806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>1.955300</td>\n",
       "      <td>2.046393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>1.936200</td>\n",
       "      <td>2.095439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>1.956600</td>\n",
       "      <td>2.071892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>1.965300</td>\n",
       "      <td>1.990853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>1.964100</td>\n",
       "      <td>2.028612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>1.960800</td>\n",
       "      <td>2.079344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>1.963900</td>\n",
       "      <td>2.078518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>1.958500</td>\n",
       "      <td>2.101017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>1.961000</td>\n",
       "      <td>2.085054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>1.951400</td>\n",
       "      <td>2.098166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>1.956200</td>\n",
       "      <td>2.120970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>1.949800</td>\n",
       "      <td>2.083788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>1.958900</td>\n",
       "      <td>2.065318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>1.960500</td>\n",
       "      <td>2.120849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>1.935400</td>\n",
       "      <td>2.009544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>1.952700</td>\n",
       "      <td>2.018563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>1.940400</td>\n",
       "      <td>2.077624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>1.937100</td>\n",
       "      <td>2.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>1.939300</td>\n",
       "      <td>2.032412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>1.956800</td>\n",
       "      <td>2.067861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>1.947200</td>\n",
       "      <td>2.116207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.create_model_card() got an unexpected keyword argument 'use_temp_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Push the model to Hugging Face hub\u001b[39;00m\n\u001b[1;32m     46\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(new_model)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_temp_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:475\u001b[0m, in \u001b[0;36mSFTTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mOverwrite the `push_to_hub` method in order to force-add the tag \"sft\" when pushing the\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03mmodel on the Hub. Please refer to `~transformers.Trainer.push_to_hub` for more details.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    473\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m trl_sanitze_kwargs_for_tagging(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, tag_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tag_names, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:4355\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, token, **kwargs)\u001b[0m\n\u001b[1;32m   4352\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model_tag \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   4353\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(model_tag)\n\u001b[0;32m-> 4355\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_card\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4357\u001b[0m \u001b[38;5;66;03m# Wait for the current upload to be finished.\u001b[39;00m\n\u001b[1;32m   4358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish_current_push()\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.create_model_card() got an unexpected keyword argument 'use_temp_dir'"
     ]
    }
   ],
   "source": [
    "# Set training hyperparameters with checkpointing\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=4,  # Increase batch size for RTX 3090\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,  # Adjust gradient accumulation\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=3,  # Increase number of epochs to 3\n",
    "    eval_strategy=\"steps\",  # Evaluate on steps\n",
    "    save_strategy=\"steps\",  # Save checkpoints on steps\n",
    "    eval_steps=1000,  # Evaluate every 1000 steps\n",
    "    logging_steps=500,  # Log training metrics every 500 steps\n",
    "    warmup_steps=100,  # Warmup steps for stability\n",
    "    learning_rate=1e-4,  # Lower learning rate for stability\n",
    "    fp16=True,  # Use mixed precision training\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    save_total_limit=2,  # Keep last 2 checkpoints\n",
    "    save_steps=2000,  # Save checkpoints every 2000 steps\n",
    "    report_to=\"wandb\",  # Report to W&B for monitoring\n",
    "    load_best_model_at_end=True,  # Load best model at the end of training\n",
    ")\n",
    "\n",
    "# Initialize the trainer with checkpointing\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,  # Adjusted max_seq_length here\n",
    "    dataset_text_field=\"text\",  # Use dataset_text_field directly\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[\n",
    "        RandomSubsetEvalCallback(eval_dataset, 500),  # Evaluate on 500 rows\n",
    "    ],\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "model.config.use_cache = False  # Disable cache during training\n",
    "trainer.train()\n",
    "model.config.use_cache = True  # Re-enable cache after training\n",
    "\n",
    "# Push the model to Hugging Face hub\n",
    "trainer.save_model(new_model)\n",
    "trainer.push_to_hub(new_model, use_temp_dir=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1ab9b7-365f-45a2-a740-3602e54d2919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66af08b55aa3443eaa87e360044c6f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cf6dc109664afa82bc843fe8de6c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0ce2984d0343a79a2e58b8edfad30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47304c87477b49d0a92db8342420ab3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87a13fc92424d63bd46f192879138b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/adamfendri/Gemma-2-2b-it-medical/commit/cf7ffeb37fd5a566f87795e35e9d0d7353e7f2de', commit_message='Upload tokenizer', commit_description='', oid='cf7ffeb37fd5a566f87795e35e9d0d7353e7f2de', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the model to Hugging Face hub\n",
    "trainer.save_model(new_model)\n",
    "trainer.push_to_hub(commit_message=\"Finetuned Gemma-2 model for medical chat\", blocking=True)\n",
    "\n",
    "# Push the tokenizer to Hugging Face hub\n",
    "tokenizer.push_to_hub(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee529c50-eba6-4a68-8bff-72cd6d47b24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI Medical Assistant trained on a vast dataset of health information. Please be thorough and provide an informative answer. If you don't know the answer to a specific medical inquiry, advise seeking professional help.\n",
      "I'm a 35-year-old male and for the past few months, I've been experiencing fatigue, increased sensitivity to cold, and dry, itchy skin. Could these symptoms be related to hypothyroidism? If so, what steps should I take to get a proper diagnosis and discuss treatment options?\n",
      "Hi, Welcome to Health care magic forum.                      As you describe it appears to be the hypothyriodism, as the symptoms are similar to that.                       I advise you to consult an endocrinologist for diagnosis,and treatment. You may need to have thyroid function tests for confirmation.                        Take more of green leafy vegetables, pulses, sprouts, protein rich foods,to have a good health and resistance against infections.                         Wishing for a quick and complete recovery. Thank you\n",
      "Dr.J.S.Singha\n",
      "Hello,Thanks for posting your query.I have gone through your question in detail and I can understand what you are going through.The symptoms that you have mentioned are suggestive of Hypothyroidism.You should get your thyroid profile done to confirm the diagnosis.If it is confirmed then you will have to take Levothyroxin tablets for life long.Hope this helps.Regards.Dr Saurabh Gupta.Orthopaedic Surgeon\n",
      "Thanks.For further questions you can consult me at healthcaremagic at the following link.  You can ask a direct question to me by typing my name as a query question. I will get back to you within 24 hours.www.healthcaremagic.com/doctors/dr-saurabh-gupt/67696\n",
      "Dear Sir,Welcome to HCM,I am Dr.R.K.Madama, Infectious Disease Specialist.Let me try to answer your questions one by one.1.Fatigue, cold sensitivity, dry skin- these are symptoms of hypothroidism.2.Hypothyroidism is a condition in which the thyroid gland does not produce enough thyroid hormone. This hormone is needed to keep your body's metabolism and energy levels normal.3.Thyroid function test (TFT) is used to check the level of thyroid hormones in the blood.4.Yes, you should consult a physician and get the tests done.5.Levothryoxine is the drug of choice.6.\n"
     ]
    }
   ],
   "source": [
    "# Example inference\n",
    "model.eval()  # Set model to evaluation mode\n",
    "prompt = '''You are an AI Medical Assistant trained on a vast dataset of health information. Please be thorough and provide an informative answer. If you don't know the answer to a specific medical inquiry, advise seeking professional help.'''\n",
    "question = '''I'm a 35-year-old male and for the past few months, I've been experiencing fatigue, increased sensitivity to cold, and dry, itchy skin. Could these symptoms be related to hypothyroidism? If so, what steps should I take to get a proper diagnosis and discuss treatment options?'''\n",
    "input_text = prompt + \"\\n\" + question\n",
    "\n",
    "# Tokenize input text for inference\n",
    "inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Generate a response from the model\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=512,  # Set max generation length\n",
    "        num_beams=5,  # Use beam search with 5 beams\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "\n",
    "# Decode and print the generated response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852984e4-ef7f-4f02-9094-2b878114005a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
